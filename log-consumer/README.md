# Mystical log consumer server

The Log Consumer Server serves as a critical component of centralized logging infrastructure, enabling seamless ingestion of logs from Kafka topics. Leveraging Kafka's distributed messaging system, the consumer server fetches logs produced during project scanning, build, and deployment processes from diverse infrastructure sources. It then stores these logs in AWS DynamoDB, providing developers with a scalable and reliable storage solution for log data.

#### `Consuming Logs from Kafka Topics:`
During project scanning, build, and deployment phases, logs are generated by various tools and services, including Sonar cli, bearer cli  and deployment platforms. These logs are published to Kafka topics, where they await consumption by the API Log Kafka Consumer Server. By subscribing to relevant Kafka topics, the consumer server retrieves logs in real-time, ensuring timely access to critical information.

#### `Storing Logs in AWS DynamoDB:`
Upon consuming logs from Kafka topics, the API Log Kafka Consumer Server stores them in AWS DynamoDBâ€”a fully managed NoSQL database service. DynamoDB offers seamless scalability, high availability, and low-latency access to log data, making it an ideal choice for storing large volumes of logs. By leveraging DynamoDB's flexible data model and powerful query capabilities, developers can efficiently retrieve and analyze log data for monitoring and troubleshooting purposes.

#### `Benefits of Using an API Log Kafka Consumer Server:`

  - `Real-time Log Ingestion:`
     Logs are consumed from Kafka topics in real-time, ensuring timely access to critical information.
  - `Scalable Storage:`
     Log data is stored in AWS DynamoDB, providing developers with a scalable and reliable storage solution for log data.
  - `High Availability:`
     DynamoDB offers high availability and durability, ensuring that log data is always accessible and resilient to failures.
  - `Simplified Monitoring and Troubleshooting:`
     By centralizing log data in DynamoDB, developers can streamline monitoring and troubleshooting efforts, leading to faster 
     issue resolution and improved operational efficiency.

### Follow the instruction step by step
Before go to step you should be install node in your system and. i used node 20.x version.


#### Step 1: Define the AWS credentials in .env file
```
AWS_ACCESS_KEY=
AWS_SECRET_KEY=
```

#### Step 2: Define the Kafka user, password and broker information in .env file
```
KAFKA_USER=
KAFKA_PASSWORD=
KAFKA_BROKERS=
```

#### Step 3: Define the REDIS path in .env file
```
REDIS_URL=
```

#### Step 4: Move project root and Install dependency
```sh
cd project-root && npm install
```

#### Step 5: Run server
```sh
npm start
```


In conclusion, the Log Consumer Server serves as a cornerstone of modern centralized logging infrastructure, enabling seamless ingestion of logs from Kafka topics and storage in AWS DynamoDB. By consolidating logs generated during project scanning, build, and deployment processes, the consumer server empowers developers with real-time visibility, scalable storage, and simplified monitoring and troubleshooting capabilities. As organizations strive to optimize development operations and deliver high-quality software products, investing in an API Log Kafka Consumer Server becomes imperative for achieving operational excellence and driving innovation.
